{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03dcd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install \"langchain==0.2.11\"\n",
    "!pip install \"langchain-core==0.2.43\"\n",
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36372fa5",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7176d35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Basic libraries\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Formatting\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Import Google Generative AI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "\n",
    "# LangChain Components\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "\n",
    "# usually we start loading with environment variables\n",
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc4079",
   "metadata": {},
   "source": [
    "# API Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e7cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60cbde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(prompt_txt, params=None):\n",
    "\n",
    "    model_id = \"gemini-2.0-flash\"\n",
    "\n",
    "    default_params = {\n",
    "        \"temperature\" : 0,\n",
    "        \"max_tokens\" : None, \n",
    "        \"timeout\" : None, \n",
    "        \"max_retries\" : 2\n",
    "    }\n",
    "\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    # Create LLM directly\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model = model_id, \n",
    "        temperature = default_params[\"temperature\"], \n",
    "        max_tokens = default_params[\"max_tokens\"], \n",
    "        timeout = default_params[\"timeout\"], \n",
    "        max_retries = default_params[\"max_retries\"]\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt_txt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cceb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'params' provided to ChatGoogleGenerativeAI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'adore la programmation.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--3d09e71f-8138-4d60-91b5-f8d68b74b030-0' usage_metadata={'input_tokens': 19, 'output_tokens': 7, 'total_tokens': 26, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "ai_msg = llm_model(prompt_txt = [(\"system\", \"You are a helpful assistant that translates English to French. Translate the user sentense\"), \n",
    "    (\"human\", \"I love programming\")])\n",
    "print(ai_msg)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66bf0a3",
   "metadata": {},
   "source": [
    "# Prompt Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a90cc1",
   "metadata": {},
   "source": [
    "#### 1. Basic Prompt\n",
    "\n",
    "A basic prompt is the simplest form of prompting, where you provide a short text or phrase to the model without any special formatting or instructions. The model generates a continuation based on patterns it has learned during training. Basic prompts are useful for exploring the model's capabilities and understanding how it naturally responds to minimal input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d13487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt : The wind is  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The wind is **blowing**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_tokens\" : 128, \n",
    "    \"temperature\" : 0.5, \n",
    "    \"max_retries\" : 2\n",
    "}\n",
    "\n",
    "prompt = \"The wind is \"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt : {prompt} \\n\")\n",
    "print(f\"response : {display(Markdown(response.content))} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4786deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt : The future of artificial intellgence is \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The future of artificial intelligence is a topic of much discussion and speculation, with potential impacts across nearly every aspect of human life. Here's a breakdown of some key areas:\n",
       "\n",
       "**Potential Positive Impacts:**\n",
       "\n",
       "*   **Automation and Efficiency:** AI can automate repetitive tasks, freeing up humans for more creative and strategic work. This could lead to increased productivity and economic growth.\n",
       "*   **Healthcare Advancements:** AI can assist in diagnosis, drug discovery, personalized medicine, and robotic surgery, potentially leading to better patient outcomes and more efficient healthcare systems.\n",
       "*   **Improved Decision-Making:** AI algorithms can analyze vast amounts of data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : None \n",
      "\n",
      "prompt : Once upon a time in a distant galaxy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Once upon a time in a distant galaxy, far, far beyond the swirling nebulae of known space, existed a planet called Xylos. Xylos wasn't a typical planet of verdant forests or shimmering oceans. Instead, it was a world sculpted entirely from crystal. Towering spires of amethyst pierced the cerulean sky, while canyons of glittering quartz snaked across the landscape. The very air hummed with a low, resonant frequency, a song of the crystals themselves.\n",
       "\n",
       "The inhabitants of Xylos were the Luminians, beings of pure light and energy, housed within crystalline shells. They were not born"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : None \n",
      "\n",
      "prompt : The benefits of sustainable energy include \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The benefits of sustainable energy are numerous and span environmental, economic, and social aspects. Here's a breakdown:\n",
       "\n",
       "**Environmental Benefits:**\n",
       "\n",
       "*   **Reduced Greenhouse Gas Emissions:** This is the most significant benefit. Sustainable energy sources like solar, wind, hydro, and geothermal produce little to no greenhouse gases during operation, mitigating climate change.\n",
       "*   **Improved Air Quality:**  Unlike fossil fuels, sustainable energy sources don't release harmful air pollutants like sulfur dioxide, nitrogen oxides, and particulate matter, leading to cleaner air and reduced respiratory problems.\n",
       "*   **Water Conservation:** Many sustainable energy technologies, particularly solar and wind"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The future of artificial intellgence is\",\n",
    "    \"Once upon a time in a distant galaxy\",\n",
    "    \"The benefits of sustainable energy include\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = llm_model(prompt, params)\n",
    "    print(f\"prompt : {prompt} \\n\")\n",
    "    print(f\"response : {display(Markdown(response.content))} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e1b9d",
   "metadata": {},
   "source": [
    "#### 2. Zero-Shot Prompts\n",
    "\n",
    "Zero-shot prompting is a technique where the model performs a task without any examples or prior specific training. This approach tests the model's ability to understand instructions and apply its knowledge to a new context without demonstration. Zero-shot prompts typically include clear instructions about what the model should do, allowing it to leverage its pre-trained knowledge effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "562bcc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt : Classify the following sentence as true or false:\n",
      "                'The Eiffel Tower is located in Berlin.'\n",
      "\n",
      "            Answer:\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"Classify the following sentence as true or false:\n",
    "                'The Eiffel Tower is located in Berlin.'\n",
    "\n",
    "            Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt : {prompt} \\n\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "273628b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======MOVIE_REVIEW RESPONSE ======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Answer: Positive\n",
       "\n",
       "Comment: The review uses words like \"thrilling,\" \"intense,\" \"superb,\" and \"highly recommended,\" all indicating a positive experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======TRANSLATE_LANG RESPONSE ======\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:** ¡Hola! ¿Cómo están, caballeros?\n",
       "\n",
       "**Comment:**\n",
       "\n",
       "*   **¡Hola!** is the standard greeting for \"Hi!\"\n",
       "*   **¿Cómo están?** is the formal \"How are you?\" in the plural. Using \"están\" is appropriate because you're addressing multiple people.\n",
       "*   **caballeros** is the Spanish word for \"gentlemen.\" It adds a polite and respectful tone."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We are creating two prompts one for to classify a movie review as positive or negative and second one to translate an English phrase to Spanish.\n",
    "\n",
    "movie_review_prompt = \"\"\"Classify a movie review as positive or negative:\n",
    "\n",
    "                        'F1 movie : This movie was a thrilling ride from start to finish! The racing scenes were intense, the acting was superb, and the story kept me on the edge of my seat. Highly recommended!'\n",
    "\n",
    "                        Answer : \n",
    "                        \n",
    "                        Comment :\n",
    "\"\"\"\n",
    "\n",
    "translate_prompt = \"\"\"\n",
    "                Translate the following sentence from English phrase to Spanish\n",
    "                'Hi, How are doing gentlemen!!'\n",
    "\n",
    "                Answer: \n",
    "                Comment : \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompts = {}\n",
    "prompts[\"movie_review\"] = movie_review_prompt\n",
    "prompts[\"translate_lang\"] = translate_prompt\n",
    "\n",
    "for prompt_type, prompt in prompts.items():\n",
    "    print(f\"======={prompt_type.upper()} RESPONSE ======\")\n",
    "    response = llm_model(prompt, params)\n",
    "    display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71239297",
   "metadata": {},
   "source": [
    "#### 3. Few-shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc529d34",
   "metadata": {},
   "source": [
    "Few-shot prompting extends the one-shot approach by providing multiple examples (typically 2-5) before asking the model to perform the task. These examples establish a clearer pattern and context, helping the model better understand the expected output format, style and reasoning. This technique is particularly effective for complex tasks where a single example might not convey all the nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05d4e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt : Here are few examples for classifying emotions in statements: \n",
      "\n",
      "        Statement: 'I just won my first marathon!'\n",
      "        Emotion: Joy\n",
      "\n",
      "        Statement: 'I can't believe I lost my keys again.'\n",
      "        Emotion: Frustation\n",
      "\n",
      "        Statement: 'My best friend is moving to another country.'\n",
      "        Emotion: Sadness\n",
      "\n",
      "        Now, classify the emotion in the following statement:\n",
      "        Statement: 'That movie was so scary I had to cover my eyes.'\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Emotion: Fear"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_tokens\" : 10,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Here are few examples for classifying emotions in statements: \n",
    "\n",
    "        Statement: 'I just won my first marathon!'\n",
    "        Emotion: Joy\n",
    "\n",
    "        Statement: 'I can't believe I lost my keys again.'\n",
    "        Emotion: Frustation\n",
    "\n",
    "        Statement: 'My best friend is moving to another country.'\n",
    "        Emotion: Sadness\n",
    "\n",
    "        Now, classify the emotion in the following statement:\n",
    "        Statement: 'That movie was so scary I had to cover my eyes.'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt : {prompt} \\n\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f323b7f",
   "metadata": {},
   "source": [
    "#### 4. Chain-of-thought (CoT) prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd6477",
   "metadata": {},
   "source": [
    "Chain-of-thought (CoT) prompting encourages the model to break down complex problems into step-by-step reasoning before arriving at a final answer. By explicitly showing or requesting intermediate steps, this technique improves the model's problem-solving abilities and reducess errors in tasks requiring multi-step reasoning. CoT is particularly effective for mathematical problems, logical reasoning, and complex decision-making tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "152146ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples.\n",
      "            How many apples are there now?'\n",
      "\n",
      "    Break down each step of your calculation\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's the breakdown of the calculation:\n",
       "\n",
       "1. **Start:** The store begins with 22 apples.\n",
       "\n",
       "2. **Sold:** They sell 15 apples, so we subtract that from the starting amount: 22 - 15 = 7 apples.\n",
       "\n",
       "3. **Delivery:** They receive a delivery of 8 new apples, so we add that to the remaining amount: 7 + 8 = 15 apples.\n",
       "\n",
       "**Therefore, the store now has 15 apples.**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples.\n",
    "            How many apples are there now?'\n",
    "\n",
    "    Break down each step of your calculation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt} \\n\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "917ba7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples.\n",
      "            How many apples are there now?'\n",
      "\n",
      "            Answer: \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Answer: 15"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"max_new_tokens\": 512, \n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples.\n",
    "            How many apples are there now?'\n",
    "\n",
    "            Answer: \n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt} \\n\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a76eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt :  Consider the situation: 'Whether a student should study tonight\n",
      "or go to a movie with friends, considering their upcoming test in two days.\n",
      "Can you summarize the answer in 250 words and provide your decision'\n",
      "\n",
      "Break down your reason:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, here's a breakdown and summary of the decision:\n",
       "\n",
       "**Reasoning Breakdown:**\n",
       "\n",
       "*   **Academic Priority:** The upcoming test in two days is a significant factor. Academic performance is crucial for long-term goals.\n",
       "*   **Time Management:** Two days provides some time for both studying and leisure. The key is to allocate time effectively.\n",
       "*   **Study Effectiveness:** Cramming the night before a test is often less effective than spaced repetition. A focused study session tonight, followed by a lighter review tomorrow, might be optimal.\n",
       "*   **Social Well-being:** Social interaction is important for mental health and can reduce stress. Completely isolating oneself before a test can be counterproductive.\n",
       "*   **Opportunity Cost:** Choosing the movie means sacrificing study time. Choosing to study means missing out on social time with friends.\n",
       "\n",
       "**Decision:**\n",
       "\n",
       "The student should **study tonight, but with a compromise.**\n",
       "\n",
       "**Summary (within 250 words):**\n",
       "\n",
       "The student faces a classic dilemma: academics versus social life. While the upcoming test is important, completely sacrificing social interaction isn't ideal. The best approach is a balanced one. The student should dedicate a focused block of time (e.g., 2-3 hours"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params\n",
    "params = {\n",
    "    \"max_tokens\" : 256\n",
    "}\n",
    "\n",
    "# Prompt for decision-making process\n",
    "decision_making_prompt = \"\"\" Consider the situation: 'Whether a student should study tonight\n",
    "or go to a movie with friends, considering their upcoming test in two days.\n",
    "Can you summarize the answer in 250 words and provide your decision'\n",
    "\n",
    "Break down your reason:\n",
    "\n",
    "\"\"\"\n",
    "response = llm_model(decision_making_prompt, params)\n",
    "print(f\"prompt : {decision_making_prompt}\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f292bd",
   "metadata": {},
   "source": [
    "#### 6. Self-consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077867ac",
   "metadata": {},
   "source": [
    "Self-consistency is an advanced technique in which the model generates multiple independent solutions or answers to the same problem, then evaluates these different approaches to determine most consistent or reliable result. This method enhances accuracy by leverarging the model's ability to approach problems from different angles and identify the most robust solution through comparison and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1eb197b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:  When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
      "\n",
      "Provide three independent calculations and explanations, then determine the most consistent result.\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are three independent calculations to determine your sister's age, along with explanations, and a final determination:\n",
       "\n",
       "**Calculation 1: Age Difference**\n",
       "\n",
       "*   **Explanation:** The age difference between you and your sister remains constant throughout your lives.\n",
       "*   **Calculation:**\n",
       "    *   When you were 6, your sister was 6 / 2 = 3 years old.\n",
       "    *   The age difference is 6 - 3 = 3 years.\n",
       "    *   Therefore, your sister is always 3 years younger than you.\n",
       "    *   Now that you are 70, your sister is 70 - 3 = 67 years old.\n",
       "\n",
       "**Calculation 2: Relative Age Increase**\n",
       "\n",
       "*   **Explanation:** We can track how many years have passed since the initial age difference was established.\n",
       "*   **Calculation:**\n",
       "    *   When you were 6, your sister was 3.\n",
       "    *   Years passed: 70 - 6 = 64 years.\n",
       "    *   Sister's age now: 3 + 64 = 67 years old.\n",
       "\n",
       "**Calculation 3: Ratio Method**\n",
       "\n",
       "*   **Explanation:** While the ratio of your ages changes, the *difference* remains constant. We can use the initial ratio to confirm the age difference.\n",
       "*   **Calculation:**\n",
       "    *   When you were 6, the ratio of your age to your sister's age was 2:1.\n",
       "    *   This confirms the age difference of 3 years (6 - 3 = 3).\n",
       "    *   Since you are now 70, your sister is 70 - 3 = 67 years old.\n",
       "\n",
       "**Determination:**\n",
       "\n",
       "All three calculations consistently result in your sister being **67 years old**. Therefore, this is the most consistent and likely correct answer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    'max_tokens': 512,\n",
    "}\n",
    "\n",
    "prompt = \"\"\" When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
    "\n",
    "Provide three independent calculations and explanations, then determine the most consistent result.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(f\"prompt: {prompt} \\n\")\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5198c61",
   "metadata": {},
   "source": [
    "# Now, we will explore LCEL Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f89ca",
   "metadata": {},
   "source": [
    "Five steps that is common while building application using LCEL approach:\n",
    "\n",
    "1. Define the content or problem to be addressed.\n",
    "2. Create a template with variables for dynamic content.\n",
    "3. Convert the template into a LangChain PromptTemplate.\n",
    "4. Build a chain using the pipe operator | to connect.\n",
    "    1. Input Variables\n",
    "    2. The prompt template\n",
    "    3. The LLM\n",
    "    4. An output parser\n",
    "\n",
    "5. Invoke the chain with specific inputs to generate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b2bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x122574d70>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"gemini-2.0-flash\"\n",
    "\n",
    "default_params = {\n",
    "    \"temperature\" : 0,\n",
    "    \"max_tokens\" : None, \n",
    "    \"timeout\" : None, \n",
    "    \"max_retries\" : 2\n",
    "}\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = model_id,\n",
    "    temperature = default_params[\"temperature\"],\n",
    "    max_tokens = default_params[\"max_tokens\"],\n",
    "    timeout = default_params[\"timeout\"],\n",
    "    max_retries = default_params[\"max_retries\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a30bfa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Tell me a {adjective} joke about {content}.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3304df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chicken.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, let's take a look how the prompt has been formatted\n",
    "prompt.format(adjective = \"funny\", content = \"chicken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ca610",
   "metadata": {},
   "source": [
    "To ensure consistent formatting of the prompts, we will define a helper function format_prompt. This function takes a dictionary of variables and applies them to our prompt template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2926d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to ensure proper formatting\n",
    "def format_prompt(variables):\n",
    "    return prompt.format(**variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21beab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the chicken cross the playground?\n",
       "\n",
       "To get to the other slide.\n",
       "\n",
       "...But he was too late. They'd already closed it down because of the avian flu. Now he's just standing there, staring at the empty slide, wondering what could have been."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following code builds a chain using LCEL(LangChain Expression Language). This\n",
    "# chain connects components using the pipe operator (|) to create a processing flow.\n",
    "# The chain takes input variables, passes them through the prompt template, sends the \n",
    "# formatted prompt to the LLM, and uses a string output parser to return the final responses.\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "joke_chain = (\n",
    "    RunnableLambda(format_prompt)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = joke_chain.invoke({\"adjective\":\"sad\", \"content\":\"chicken\"})\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22e6f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the human cross the road?\n",
       "\n",
       "Because they saw something shiny on the other side and completely forgot what they were doing in the first place. Then they got distracted by a squirrel."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can now just simply replace the varibles accordingly for other situation\n",
    "response = joke_chain.invoke({\"adjective\":\"funny\", \"content\":\"human being\"})\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6a5fb",
   "metadata": {},
   "source": [
    "#### 1. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "958bde81",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Summarize the {content} in two sentence\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# helper function for prompt template\n",
    "def format_template(variables):\n",
    "    return prompt.format(**variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0550dbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Rapid technological advancements like AI, machine learning, and IoT are revolutionizing industries like healthcare, education, and transportation. These innovations are enhancing productivity, improving accessibility, and fostering a more interconnected and informed global society."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "content = \"\"\"\n",
    "    The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n",
    "    Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n",
    "    For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n",
    "    Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n",
    "    These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
    "\"\"\"\n",
    "\n",
    "summary_chain = (\n",
    "    RunnableLambda(format_template)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summary = summary_chain.invoke({\"content\":content})\n",
    "display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af639e3",
   "metadata": {},
   "source": [
    "#### 2. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "    The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "    The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "    The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "\n",
    "template = \"\"\"\n",
    "    Answer the {question} based on the {content}.\n",
    "    Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "\n",
    "    Answer:\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99922cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
